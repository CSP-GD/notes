{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "00",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAAUCYst0Cue",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CSP-GD/notes/blob/master/practice/library/tensorflow/%E5%9C%A8colab%E9%96%8B%E5%A7%8B%E5%AD%B8%E8%B5%B7tf/00/00.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbuqDGE5m3Oa",
        "colab_type": "text"
      },
      "source": [
        "# TensorFlow 2.x 學習筆記--開篇 \n",
        "\n",
        "> 這系列筆記是由某位萌新一時興起，  \n",
        "> 從 tensorflow 2.x 去學習 DL的角度所編寫而成，  \n",
        "> 希望能帶給各位讀者一些小小的收穫。  \n",
        "> by.萌新  \n",
        "> \n",
        "> ps. 本系列會不定期的棄坑與補坑  \n",
        "> pps. 想從 keras 開始的讀者請去 [此處](https://www.google.com/?hl=zh-tw) 尋找中意的教學。 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpTMX5iTqlAu",
        "colab_type": "text"
      },
      "source": [
        "## 關於 [TensorFlow](https://www.tensorflow.org/)\n",
        "  \n",
        "  \n",
        "1. **高維陣列運算**  \n",
        "   能使用gpu對矩陣運算進行加速的數學函式庫，  \n",
        "   並包含了許多矩陣的操作方法。 \n",
        "2. **向後自動微分**  \n",
        "   藉由將運算式解析成多個可微分算子並使用已定義好的導數規則，  \n",
        "   就能在正向算出答案後反向推導出微分結果。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eNE-Yj4COfp",
        "colab_type": "text"
      },
      "source": [
        "### TensorFlow 2.x 與 1.x 的不同之處\n",
        "\n",
        "**面向動態計算圖的API操作方式**  \n",
        "  \n",
        "在 1.x 時期，TensorFlow 都是以靜態計算圖為主體，  \n",
        "那時編寫 tf 會以計算的順序創建出一張計算圖，  \n",
        "再透過 session 真正執行運算，使用起來不大直觀，  \n",
        "直到後期推出的 eager 模式才出現改變。\n",
        "  \n",
        "而 2.x 則改變過往，將動態圖做為預設，  \n",
        "如此就能讓使用者具有像是平常編寫程式一樣的直觀體驗，  \n",
        "完成模型後，只需在函數前面加上 `@tf.function`，  \n",
        "tf 2.x 就會幫你將函數轉變成靜態圖並執行最佳化。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFPJ2ga8LXsK",
        "colab_type": "text"
      },
      "source": [
        "## 小型範例\n",
        "  \n",
        "> 參考於 [此處](https://wellwind.idv.tw/blog/2018/04/07/tensorflow-js-basic/)  \n",
        "  \n",
        "本篇範例將藉由模擬乘法函數來介紹 tf 的基礎概念"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DjSTWEQGNUrU"
      },
      "source": [
        "### 導入 Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBz0ob_Fq53F",
        "colab_type": "code",
        "outputId": "af1a4be2-c84e-494f-d171-ee3adb22ffd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0mqYMNpI-VQ",
        "colab_type": "text"
      },
      "source": [
        "### 生成資料  \n",
        "  \n",
        "1. [`tf.random`](https://www.tensorflow.org/api_docs/python/tf/random) 中定義了各種亂數生成方法，在此使用 `normal` 生成亂數  \n",
        "2. [`tf.math`](https://www.tensorflow.org/api_docs/python/tf/math) 中定義了基本運算包含 `add`、`multiply`、`abs` 等等，  \n",
        "   也有需多基本運算可直接從 tf 底下調用，或是有實作運算子重載。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfsz8y-04FEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_data(data_num, answer):\n",
        "    x = tf.random.normal((data_num,1))\n",
        "\n",
        "    # 從 tf.math 下調用 multiply\n",
        "    y = tf.math.multiply(x, answer)\n",
        "\n",
        "    # 從 tf 下直接調用 multiply\n",
        "    # y = tf.multiply(x, answer)\n",
        "\n",
        "    # 使用重載過的運算子\n",
        "    # y = x * answer\n",
        "\n",
        "    return [x,y]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxNY1Ur6M2jW",
        "colab_type": "text"
      },
      "source": [
        "### 訂定函數\n",
        "  \n",
        "1. [`@tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) 代表要讓 tf 將此函數轉換成計算圖並進行最佳化  \n",
        "2. [`tf.linalg`](https://www.tensorflow.org/api_docs/python/tf/linalg) 為線性代數的函示庫，內含有 `matmul`、`qr` 等等，  \n",
        "   其中有部分函數可從 tf 下直接調用，或是有實作運算子重載。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2q_UWE6Mygj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def model(w,x):\n",
        "    # 從 tf.linalg 下調用 matmul\n",
        "    return tf.linalg.matmul(x, w)\n",
        "    \n",
        "    # 從 tf 下直接調用 matmul\n",
        "    # return tf.matmul(x, w)\n",
        "    \n",
        "    # 使用重載過的運算子\n",
        "    # return x @ w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW5dzSGuueyc",
        "colab_type": "text"
      },
      "source": [
        "### 訓練流程\n",
        "  \n",
        "1. [`tf.optimizers`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) 中放有各種優化器，在此使用 `Adamax` 作為優化器。  \n",
        "   在計算出梯度後使用 `apply_gradients` 改變權重  \n",
        "2. [`tf.losses`](https://www.tensorflow.org/api_docs/python/tf/keras/losses) 中放有各式各樣的損失函數，在此使用 `mean_squared_error` 計算 loss。  \n",
        "> **$MSE = \\frac{1}{n}\\sum_{i=1}^n(y_i-y_i)^2$**  \n",
        "3. 在 [`with tf.GradientTape() as tape:`](https://www.tensorflow.org/api_docs/python/tf/GradientTape) 下的運算將會被記錄下來，  \n",
        "   之後再使用 `tape.gradient(loss, weights)` 計算出權重的梯度。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-CHGPZU60qZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, w, x, y, epoch, lr=0.001):\n",
        "    optimizer = tf.optimizers.Adamax(lr)\n",
        "    for i in range(epoch):\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(w, x)\n",
        "            loss = tf.losses.mean_squared_error(y_pred=y_pred, y_true=y)\n",
        "        grads = tape.gradient(loss, [w])\n",
        "        optimizer.apply_gradients(zip(grads, [w]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvP5YRenM39D",
        "colab_type": "text"
      },
      "source": [
        "### 開始訓練\n",
        "- [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) 製作 tensor 變數，  \n",
        "  比較重要的參數有：  \n",
        "  1. initial_value : 初始值\n",
        "  2. trainable(預設為 True) : 是否可被 `optimizer` 更新數值\n",
        "  3. shape(預設為 initial_value 的維度大小) : tensor 的維度大小\n",
        "  4. dtype(預設為 initial_value 的型別) : tensor 的型別，詳細可見 [`tf.dtypes`](https://www.tensorflow.org/api_docs/python/tf/dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "choXx1oXnZkz",
        "colab_type": "code",
        "outputId": "de4d0b42-3c10-44ac-92ad-210e06ece806",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "w = tf.Variable(tf.random.normal([1,1]))\n",
        "tf.dtypes.\n",
        "[x, y] = generate_data(10000, 20)\n",
        "\n",
        "print(\"w : \", w.value().numpy())\n",
        "\n",
        "for i in range(5):\n",
        "    train(model, w, x, y, 100, 0.1)\n",
        "    print(\"loss : \", tf.math.reduce_mean(tf.losses.mean_squared_error(y_pred=model(w,x), y_true=y)).numpy())\n",
        "\n",
        "print(\"w : \", w.value().numpy())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w :  [[0.22930485]]\n",
            "a\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-82f04ca5dbf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-fb39d0b7792c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, w, x, y, epoch, lr)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    472\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mnone\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \"\"\"\n\u001b[0;32m--> 474\u001b[0;31m     \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_filter_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_filter_grads\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m   1201\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m     raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[0;32m-> 1203\u001b[0;31m                      ([v.name for _, v in grads_and_vars],))\n\u001b[0m\u001b[1;32m   1204\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     logging.warning(\n",
            "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: ['Variable:0']."
          ]
        }
      ]
    }
  ]
}