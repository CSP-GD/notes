{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "00",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAAUCYst0Cue",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CSP-GD/notes/blob/master/practice/library/tensorflow/%E5%9C%A8colab%E9%96%8B%E5%A7%8B%E5%AD%B8%E8%B5%B7tf/00/00.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbuqDGE5m3Oa",
        "colab_type": "text"
      },
      "source": [
        "# TensorFlow 2.x 學習筆記--開篇 \n",
        "\n",
        "> 這系列筆記是由某位萌新一時興起，  \n",
        "> 從 tensorflow 2.x 去學習 DL的角度所編寫而成，  \n",
        "> 希望能帶給各位讀者一些小小的收穫。  \n",
        "> by.萌新  \n",
        "> \n",
        "> ps. 本系列會不定期的棄坑與補坑  \n",
        "> pps. 想從 keras 開始的讀者請去 [此處](https://www.google.com/?hl=zh-tw) 尋找中意的教學。 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpTMX5iTqlAu",
        "colab_type": "text"
      },
      "source": [
        "## 關於 TensorFlow\n",
        "  \n",
        "  \n",
        "1. **高維陣列運算**  \n",
        "   能使用gpu對矩陣運算進行加速的數學函式庫，  \n",
        "   並包含了許多矩陣的操作方法。 \n",
        "2. **向後自動微分**  \n",
        "   藉由將運算式解析成多個可微分算子並使用已定義好的導數規則，  \n",
        "   就能在正向算出答案後反向推導出微分結果。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eNE-Yj4COfp",
        "colab_type": "text"
      },
      "source": [
        "### TensorFlow 2.x 與 1.x 的不同之處\n",
        "\n",
        "**面向動態計算圖的API操作方式**  \n",
        "  \n",
        "在 1.x 時期，TensorFlow 都是以靜態計算圖為主體，  \n",
        "那時編寫 tf 會以計算的順序創建出一張計算圖，  \n",
        "再透過 session 真正執行運算，使用起來不大直觀，  \n",
        "直到後期推出的 eager 模式才出現改變。\n",
        "  \n",
        "而 2.x 則改變過往，將動態圖做為預設，  \n",
        "如此就能讓使用者具有像是平常編寫程式一樣的直觀體驗，  \n",
        "完成模型後，只需在函數前面加上 `@tf.function`，  \n",
        "tf 2.x 就會幫你將函數轉變成靜態圖並執行最佳化。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFPJ2ga8LXsK",
        "colab_type": "text"
      },
      "source": [
        "## 小型範例\n",
        "  \n",
        "> 參考於 [此處](https://wellwind.idv.tw/blog/2018/04/07/tensorflow-js-basic/)  \n",
        "  \n",
        "本篇範例將藉由模擬乘法函數來介紹 tf 的基礎概念"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DjSTWEQGNUrU"
      },
      "source": [
        "### 導入 Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBz0ob_Fq53F",
        "colab_type": "code",
        "outputId": "526e88bc-33bc-4a01-8289-ec8dcc989db4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0mqYMNpI-VQ",
        "colab_type": "text"
      },
      "source": [
        "### 生成資料  \n",
        "  \n",
        "1. `tf.random` 中定義了各種亂數生成方法，在此使用 `normal` 生成亂數  \n",
        "2. `tf.math` 中定義了基本運算包含 `add`、`multiply`、`abs` 等等，  \n",
        "   也有需多基本運算可直接從 tf 底下調用，或是有實作運算子重載。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfsz8y-04FEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_data(data_num, answer):\n",
        "    x = tf.random.normal((data_num,1))\n",
        "\n",
        "    # 從 tf.math 下調用 multiply\n",
        "    y = tf.math.multiply(x, answer)\n",
        "\n",
        "    # 從 tf 下直接調用 multiply\n",
        "    # y = tf.multiply(x, answer)\n",
        "\n",
        "    # 使用重載過的運算子\n",
        "    # y = x * answer\n",
        "\n",
        "    return [x,y]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxNY1Ur6M2jW",
        "colab_type": "text"
      },
      "source": [
        "### 訂定函數\n",
        "  \n",
        "1. `@tf.function` 代表要讓 tf 將此函數轉換成計算圖並進行最佳化  \n",
        "2. `tf.linalg` 為線性代數的函示庫，內含有 `matmul`、`qr` 等等，  \n",
        "   其中有部分函數可從 tf 下直接調用，或是有實作運算子重載。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2q_UWE6Mygj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def model(w,x):\n",
        "    # 從 tf.linalg 下調用 matmul\n",
        "    return tf.linalg.matmul(x, w)\n",
        "    \n",
        "    # 從 tf 下直接調用 matmul\n",
        "    # return tf.matmul(x, w)\n",
        "    \n",
        "    # 使用重載過的運算子\n",
        "    # return x @ w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW5dzSGuueyc",
        "colab_type": "text"
      },
      "source": [
        "### 訓練流程\n",
        "  \n",
        "1. `tf.optimizers` 中放有各種優化器，在此使用 `Adamax` 作為優化器。  \n",
        "   在計算出梯度後使用 `apply_gradients` 改變權重  \n",
        "2. `tf.losses` 中放有各式各樣的損失函數，在此使用 `mean_squared_error` 計算 loss。  \n",
        "> **$MSE = \\frac{1}{n}\\sum_{i=1}^n(y_i-y_i)^2$**  \n",
        "3. `with tf.GradientTape() as tape`  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-CHGPZU60qZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, w, x, y, epoch, lr=0.001):\n",
        "    optimizer = tf.optimizers.Adamax(lr)\n",
        "    for i in range(epoch):\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(w, x)\n",
        "            loss = tf.losses.mean_squared_error(y_pred=y_pred, y_true=y)\n",
        "        grads = tape.gradient(loss, [w])\n",
        "        optimizer.apply_gradients(zip(grads, [w]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "choXx1oXnZkz",
        "colab_type": "code",
        "outputId": "835ef466-7003-4e89-9046-310c6802367b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "w = tf.Variable(tf.random.normal([1,1]))\n",
        "[x, y] = generate_data(10000, 20)\n",
        "\n",
        "print(w)\n",
        "\n",
        "for i in range(5):\n",
        "    train(model, w, x, y, 100, 0.1)\n",
        "    print(tf.reduce_mean(tf.losses.mean_squared_error(y_pred=model(w,x), y_true=y)))\n",
        "\n",
        "print(w)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.Variable 'Variable:0' shape=(1, 1) dtype=float32, numpy=array([[-0.1519583]], dtype=float32)>\n",
            "tf.Tensor(136.21323, shape=(), dtype=float32)\n",
            "tf.Tensor(19.255444, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0276108, shape=(), dtype=float32)\n",
            "tf.Tensor(1.5728537e-07, shape=(), dtype=float32)\n",
            "tf.Tensor(6.6261215e-08, shape=(), dtype=float32)\n",
            "<tf.Variable 'Variable:0' shape=(1, 1) dtype=float32, numpy=array([[20.000257]], dtype=float32)>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}