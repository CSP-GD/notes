{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fasterRCNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3ec86b1c36b34e6e8e568f779c125ad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_25fef446adb747e687e9c9fed54425d4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_751eed9134414870a70d8ffd3e53f574",
              "IPY_MODEL_0899921e035a4dffacd17fe332f6de5d"
            ]
          }
        },
        "25fef446adb747e687e9c9fed54425d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "751eed9134414870a70d8ffd3e53f574": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4e1d0eb71b724161af689131a4532ca3",
            "_dom_classes": [],
            "description": "Dl Completed...: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9d7839ee821f422d994e58c963d168d3"
          }
        },
        "0899921e035a4dffacd17fe332f6de5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3674782fd79f415982cb3b7a6cce5360",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/4 [00:07&lt;00:00,  1.91s/ file]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d5be5bc683e2470aad8ee8b764cdceba"
          }
        },
        "4e1d0eb71b724161af689131a4532ca3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9d7839ee821f422d994e58c963d168d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3674782fd79f415982cb3b7a6cce5360": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d5be5bc683e2470aad8ee8b764cdceba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS3SUIILB4Ih",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://github.com/CSP-GD/notes/blob/master/theory/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/%E7%89%A9%E4%BB%B6%E8%AD%98%E5%88%A5/faster-rcnn/fasterRCNN.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gESaBao1MZ7e",
        "colab_type": "code",
        "outputId": "f3244f27-b7e8-4997-8d39-18d156cb4615",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)\n",
        "print(tfds.__version__)\n",
        "print(np.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc1\n",
            "2.1.0\n",
            "1.18.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5oKO87OIa3X",
        "colab_type": "code",
        "outputId": "a550e60d-fb65-45c4-8955-044cf29f2c3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202,
          "referenced_widgets": [
            "3ec86b1c36b34e6e8e568f779c125ad0",
            "25fef446adb747e687e9c9fed54425d4",
            "751eed9134414870a70d8ffd3e53f574",
            "0899921e035a4dffacd17fe332f6de5d",
            "4e1d0eb71b724161af689131a4532ca3",
            "9d7839ee821f422d994e58c963d168d3",
            "3674782fd79f415982cb3b7a6cce5360",
            "d5be5bc683e2470aad8ee8b764cdceba"
          ]
        }
      },
      "source": [
        "# tfds.object_detection.voc.xml.parsers.expat\n",
        "# tfds.load('c')\n",
        "\n",
        "# Construct a tf.data.Dataset\n",
        "ds = tfds.load('mnist', split='train', shuffle_files=True)\n",
        "\n",
        "# Build your input pipeline\n",
        "ds = ds.shuffle(1024).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "for example in ds.take(1):\n",
        "  image, label = example[\"image\"], example[\"label\"]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset mnist/3.0.0 (download: 11.06 MiB, generated: Unknown size, total: 11.06 MiB) to /root/tensorflow_datasets/mnist/3.0.0...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
            "local data directory. If you'd instead prefer to read directly from our public\n",
            "GCS bucket (recommended if you're running on GCP), you can instead set\n",
            "data_dir=gs://tfds-data/datasets.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ec86b1c36b34e6e8e568f779c125ad0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Dl Completed...', max=4, style=ProgressStyle(description_widt…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DPMUlw2mvts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 參考 https://blog.csdn.net/Eddy_zheng/article/details/52126641\n",
        "\n",
        "# detection_result : [batch,4=>x1,y1,x2,y2]\n",
        "# ground_truth : [batch,4=>x1,y1,x2,y2]\n",
        "def IoU(detection_result, ground_truth):\n",
        "    def xy_wh(box):\n",
        "        _box = tf.reshape(box, [-1,2,2])\n",
        "        _box = tf.unstack(_box, axis=1)\n",
        "        _box_xy = _box[0]\n",
        "        _box_wh = tf.subtract(_box[1], _box_xy)\n",
        "        return [_box_xy, _box_wh]\n",
        "\n",
        "    [dr_xy, dr_wh] = xy_wh(detection_result)\n",
        "    dr_area = tf.reduce_prod(dr_wh, 1)\n",
        "\n",
        "    [gt_xy, gt_wh] = xy_wh(ground_truth)\n",
        "    gt_area = tf.reduce_prod(gt_wh, 1)\n",
        "\n",
        "    start = tf.minimum(dr_xy, gt_xy)\n",
        "    end = tf.maximum(tf.add(dr_xy, dr_wh), tf.add(gt_xy, gt_wh))\n",
        "    wh = tf.subtract(\n",
        "        tf.add(dr_wh, gt_wh),\n",
        "        tf.subtract(end, start)\n",
        "    )\n",
        "    wh = tf.nn.relu(wh)\n",
        "    area = tf.reduce_prod(wh, 1)\n",
        "\n",
        "    ratio = tf.divide(\n",
        "        area,\n",
        "        tf.subtract(\n",
        "            tf.add(dr_area, gt_area),\n",
        "            area\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return ratio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVbPuEBRH4Y1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_anchor(scales,radios):\n",
        "    anchor = []\n",
        "    for scale in scales:\n",
        "        for radio in radios:\n",
        "            anchor.append([scale*radio/min(radio,1),scale/min(radio,1)])\n",
        "    return anchor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MuoPLzQ1Kg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def generate_coordinate(w,h):\n",
        "    x = tf.tile(\n",
        "        tf.reshape(\n",
        "            tf.linspace(0., w-1, w),\n",
        "            [w,1]\n",
        "        ),\n",
        "        [1,h]\n",
        "    )\n",
        "    y = tf.reshape(\n",
        "        tf.tile(\n",
        "            tf.linspace(0., h-1, h),\n",
        "            [w]\n",
        "        ),\n",
        "        [w,-1]\n",
        "    )\n",
        "    return tf.stack([x,y], axis=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P7vpPN3EbHk",
        "colab_type": "text"
      },
      "source": [
        "# Region Proposal Networks (RPN)\n",
        "\n",
        "> RPN 會接受前一層特徵提取器輸出的 feature map，  \n",
        "> 並且輸出該位置的 anchor 有物件的機率以及對 anchor 的 offset，  \n",
        "> 使用 offset 應用至 anchor 後即可得出 bbox。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJlpZi39_Rc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def RPN(\n",
        "        last_channel = 512,\n",
        "        base_anchors = generate_anchor((128, 256, 512),(0.5, 1, 2))\n",
        "    ):\n",
        "    k = len(base_anchors)\n",
        "\n",
        "    conv_w_base = tf.Variable(tf.random.truncated_normal((3, 3, last_channel, last_channel)))\n",
        "    conv_b_base = tf.Variable(tf.random.truncated_normal((512,)))\n",
        "    \n",
        "    conv_w_for_object = tf.Variable(tf.random.truncated_normal((1, 1, last_channel, 2 * k)))\n",
        "    conv_b_for_object = tf.Variable(tf.random.truncated_normal((2 * k,)))\n",
        "    \n",
        "    conv_w_for_offset = tf.Variable(tf.random.truncated_normal((1, 1, last_channel, 4 * k)))\n",
        "    conv_b_for_offset = tf.Variable(tf.random.truncated_normal((4 * k,)))\n",
        "\n",
        "    @tf.function\n",
        "    def rpn(feature_map, image_size, top_k=128,bottom_k=None):\n",
        "        bottom_k = bottom_k if bottom_k else top_k\n",
        "        shape = tf.shape(feature_map)\n",
        "        factor = [image_size[0]/shape[1], image_size[1]/shape[2]]\n",
        "        \n",
        "        # 3 * 3 * last_channel 的 conv\n",
        "        base = tf.nn.bias_add(\n",
        "            tf.nn.conv2d(feature_map,conv_w_base, 1, \"same\"),\n",
        "            conv_b_base\n",
        "        )\n",
        "        base = tf.nn.relu(base)\n",
        "\n",
        "        # 1 * 1 * 2k 的 conv\n",
        "        for_object = tf.nn.bias_add(\n",
        "            tf.nn.conv2d(base, conv_w_for_object, 1, \"same\"),\n",
        "            conv_b_for_object\n",
        "        )\n",
        "        # 為做 softmax 將其 reshape 成 [batch, W * H * k, 2]\n",
        "        for_object = tf.reshape(for_object, [shape[0], shape[1] * shape[2] * k, 2])\n",
        "        # softmax\n",
        "        for_object = tf.nn.softmax(for_object, 2)\n",
        "        # 為了取出 positive anchor 與 negative anchor 的 index，先將其分解成 [batch, W * H * k] * 2\n",
        "        [positive_anchor_idx, negative_anchor_idx] = tf.unstack(for_object, axis = 2)\n",
        "        # 取得 positive anchor index\n",
        "        positive_anchor_idx = tf.nn.top_k(positive_anchor_idx, top_k)[1]\n",
        "        # 取得 negative anchor index\n",
        "        negative_anchor_idx = tf.nn.top_k(negative_anchor_idx, bottom_k)[1]\n",
        "\n",
        "        # 為了方便取得每個 batch 的 anchor，先對讓 index 對 batch 做 offset\n",
        "        # 算出對每個要 offset 多少\n",
        "        offset_of_expansion_batch = tf.expand_dims(\n",
        "            tf.range(\n",
        "                0,\n",
        "                shape[0] * shape[1] * shape[2] * k,\n",
        "                shape[1] * shape[2] * k\n",
        "            ),\n",
        "            1\n",
        "        )\n",
        "        # 將 positive anchor index reshape 成 [batch * W * H * k]\n",
        "        positive_anchor_idx = tf.reshape(\n",
        "            # 對 positive anchor index 做 offset\n",
        "            tf.add(\n",
        "                positive_anchor_idx,\n",
        "                offset_of_expansion_batch\n",
        "            ),\n",
        "            [-1]\n",
        "        )\n",
        "        # 將 negative anchor index reshape 成 [batch * W * H * k]\n",
        "        negative_anchor_idx = tf.reshape(\n",
        "        # 對 negative anchor index 做 offset\n",
        "            tf.add(\n",
        "                negative_anchor_idx,\n",
        "                offset_of_expansion_batch\n",
        "            ),\n",
        "            [-1]\n",
        "        )\n",
        "\n",
        "        # 1 * 1 * 4k 的 conv，計算出 anchor offset\n",
        "        for_offset = tf.nn.bias_add(\n",
        "            tf.nn.conv2d(base,conv_w_for_offset, 1, \"same\"),\n",
        "            conv_b_for_offset\n",
        "        )\n",
        "        # 把 anchor offset 分成「對座標」的 coordinate_offset 與「對大小」的 scale_offset\n",
        "        [coordinate_offset, scale_offset] = tf.unstack(\n",
        "            # 先 reshape 成 [batch, W, H, k, 2, 2] 比較好分解\n",
        "            tf.reshape(\n",
        "                for_offset, \n",
        "                [shape[0], shape[1], shape[2], k, 2, 2]\n",
        "            ),\n",
        "            axis=4\n",
        "        )\n",
        "\n",
        "        # 為後續計算暫時做轉置\n",
        "        coordinate_offset = tf.transpose(coordinate_offset, [0,3,1,2,4])\n",
        "        # 加上絕對座標位置\n",
        "        anchor_coordinate = tf.add(\n",
        "            coordinate_offset,\n",
        "            tf.reshape(\n",
        "                tf.multiply(\n",
        "                    tf.add(generate_coordinate(shape[1],shape[2]), 0.5),\n",
        "                    factor\n",
        "                ),\n",
        "                [1, 1, shape[1], shape[2], 2]\n",
        "            )\n",
        "        )\n",
        "        # 將先前的轉置變換回來\n",
        "        anchor_coordinate = tf.transpose(anchor_coordinate, [0,2,3,1,4])\n",
        "        \n",
        "        # 將其縮放倍率乘上anchor大小\n",
        "        anchor_scale = tf.multiply(\n",
        "            scale_offset,\n",
        "            base_anchors\n",
        "        )\n",
        "\n",
        "        # 合併成為 anchor，shape : [batch, W, H, k, 2, 2]\n",
        "        anchor = tf.stack([anchor_coordinate, anchor_scale], 4)\n",
        "        # reshape 成 [batch * W * H * k, 4]，方便取出 anchor\n",
        "        anchor = tf.reshape(anchor, [-1, 4])\n",
        "\n",
        "        # reshape 成 [batch, tok_k, 4]\n",
        "        positive_anchor = tf.reshape(\n",
        "            # 取出 positive anchor\n",
        "            tf.gather(\n",
        "                anchor,\n",
        "                positive_anchor_idx\n",
        "            ),\n",
        "            [-1, top_k, 4]\n",
        "        )\n",
        "\n",
        "        # reshape 成 [batch, bottom_k, 4]\n",
        "        negative_anchor = tf.reshape(\n",
        "            # 取出 negative anchor\n",
        "            tf.gather(\n",
        "                anchor,\n",
        "                negative_anchor_idx\n",
        "            ),\n",
        "            [-1, bottom_k, 4]\n",
        "        )\n",
        "        \n",
        "        # 回傳 anchor\n",
        "        return [positive_anchor, negative_anchor]\n",
        "    def get_weights():\n",
        "        return [\n",
        "                conv_w,\n",
        "                conv_b,\n",
        "                conv_w_for_object,\n",
        "                conv_b_for_object,\n",
        "                conv_w_for_offset,\n",
        "                conv_b_for_offset\n",
        "            ]\n",
        "\n",
        "    return [rpn, get_weights]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v39RjQEXon4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# anchor : tensor<shape = [batch, -1, 4 => x, y, w, h]>\n",
        "@tf.function\n",
        "def cull_anchor(anchor, image_size, min_size):\n",
        "    [xy, wh] = tf.split(anchor, 2, axis=2)\n",
        "\n",
        "    min_xy = tf.subtract(xy, tf.divide(wh, 2))\n",
        "    min_xy = tf.greater_equal(min_xy, [0, 0])\n",
        "\n",
        "    max_xy = tf.add(xy, tf.divide(wh, 2))\n",
        "    max_xy = tf.less_equal(max_xy, image_size)\n",
        "\n",
        "    cull_size = tf.greater_equal(wh, [0, 0])\n",
        "\n",
        "    mask = tf.reduce_all(tf.concat([min_xy, max_xy, cull_size], 2), 2)\n",
        "\n",
        "    return tf.split(\n",
        "        tf.boolean_mask(anchor, mask),\n",
        "        tf.reduce_sum(tf.cast(mask, \"int32\"), -1)\n",
        "    )\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhti_nDM5qqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# anchor : tensor<shape = [batch, -1, 4 => x, y, w, h]>\n",
        "@tf.function\n",
        "def clip_anchor(anchor, image_size, min_size):\n",
        "    [xy, wh] = tf.split(anchor, 2, axis=2)\n",
        "\n",
        "    min_xy = tf.subtract(xy, tf.divide(wh, 2))\n",
        "    min_xy = tf.maximum(min_xy, [0, 0])\n",
        "\n",
        "    max_xy = tf.add(xy, tf.divide(wh, 2))\n",
        "    max_xy = tf.minimum(max_xy, image_size)\n",
        "\n",
        "    clip_xy = tf.divide(tf.add(min_xy, max_xy), 2)\n",
        "    clip_wh = tf.subtract(max_xy, min_xy)\n",
        "\n",
        "    return tf.concat(\n",
        "        [clip_xy, clip_wh],\n",
        "        axis=2\n",
        "    )\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cKwi9uOFwEz",
        "colab_type": "text"
      },
      "source": [
        "# RoI Pooling\n",
        "> 使用 RPN 輸出的 bbox 對特徵提取器輸出的 feature map 進行裁切，  \n",
        "> 將裁切下來的部分做 pooling 變成 7 * 7 * Channel 後送出。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvgaSXjJhgIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# anchor : [batch] : tensor<shape = [-1, 4 => x, y, w, h]>\n",
        "@tf.function\n",
        "def roi_pooling(feature_map, anchor, image_size, output_shape = (7, 7)):\n",
        "    factor = [image_size[0]/feature_map.shape[1], image_size[1]/feature_map.shape[2]]\n",
        "    _feature_map = tf.unstack(feature_map, 0)\n",
        "    \n",
        "    clip_feature_maps = range(len(anchor))\n",
        "    for b in tf.range(len(anchor)):\n",
        "        [xy, wh] = tf.split(anchor[b], 2, axis=1)\n",
        "\n",
        "        starts = tf.subtract(xy, tf.divide(wh, 2))\n",
        "        starts = tf.floor(tf.divide(starts, factor))\n",
        "        starts = tf.unstack(tf.subtract(starts, tf.divide(wh, 2)), 0)\n",
        "\n",
        "        sizes = tf.ceil(tf.divide(wh, factor))\n",
        "        sizes = tf.unstack(sizes, 0)\n",
        "        \n",
        "        clip_feature_maps[b] = range(len(anchor[b],shape[0]))\n",
        "        for a in tf.range(len(anchor[b],shape[0])):\n",
        "            clip_feature_maps[b][a] = tf.image.resize(\n",
        "                tf.slice(_feature_map[b], starts[a], sizes[a]),\n",
        "                output_shape\n",
        "            )\n",
        "        clip_feature_maps[b] = tf.stack(clip_feature_maps[b], 0)\n",
        "    return clip_feature_maps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkVvVd5fHpYC",
        "colab_type": "text"
      },
      "source": [
        "# 分類與再次調整 bbox\n",
        "\n",
        "> 對從 RoI pooling 取得的 feature map 做 conv 與 dense，  \n",
        "> 取得該 feature map 的分類以及再次對 bbox 做 offset，  \n",
        "> 對 bbox 使用 offset 便能取得最終的 bbox"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij9dqeTdr-vp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 分類與回歸\n",
        "def Pred(\n",
        "        input_shape = (7, 7, 512),\n",
        "        class_number\n",
        "    ):\n",
        "    k = len(base_anchors)\n",
        "\n",
        "    conv_w_base_1 = tf.Variable(tf.random.truncated_normal((input_shape[0], input_shape[1], input_shape[2], input_shape[2])))\n",
        "    conv_b_base_1 = tf.Variable(tf.random.truncated_normal((input_shape[2],)))\n",
        "    \n",
        "    conv_w_base_2 = tf.Variable(tf.random.truncated_normal((1, 1, input_shape[2], input_shape[2])))\n",
        "    conv_b_base_2 = tf.Variable(tf.random.truncated_normal((input_shape[2],)))\n",
        "\n",
        "    conv_w_classtify = tf.Variable(tf.random.truncated_normal((1, 1, input_shape[2], class_number)))\n",
        "    conv_b_classtify = tf.Variable(tf.random.truncated_normal((class_number,)))\n",
        "    \n",
        "    conv_w_bbox = tf.Variable(tf.random.truncated_normal((1, 1, input_shape[2], 4 * class_number)))\n",
        "    conv_b_bbox = tf.Variable(tf.random.truncated_normal((4 * class_number,)))\n",
        "\n",
        "    # feature_map : [batch] : tensor<shape = [anchor number, ...input_shape]>\n",
        "    @tf.function\n",
        "    def pred(feature_map, image_size, top_k=128,bottom_k=None):\n",
        "        bottom_k = bottom_k if bottom_k else top_k\n",
        "        shape = tf.shape(feature_map)\n",
        "        factor = [image_size[0]/shape[1], image_size[1]/shape[2]]\n",
        "        \n",
        "        # 3 * 3 * last_channel 的 conv\n",
        "        base_1 = tf.nn.bias_add(\n",
        "            tf.nn.conv2d(feature_map, conv_w_base_1, 1),\n",
        "            conv_b_base_1\n",
        "        )\n",
        "        base_1 = tf.nn.relu(base_1)\n",
        "\n",
        "        base_2 = tf.nn.bias_add(\n",
        "            tf.nn.conv2d(base_1, conv_w_base_2, 1),\n",
        "            conv_b_base_2\n",
        "        )\n",
        "        base_2 = tf.nn.relu(base_2)\n",
        "\n",
        "        classtify = tf.nn.bias_add(\n",
        "            tf.nn.conv2d(base_2, conv_w_classtify, 1),\n",
        "            conv_b_classtify\n",
        "        )\n",
        "        classtify = tf.nn.softmax(classtify, -1)\n",
        "\n",
        "        bbox = tf.nn.bias_add(\n",
        "            tf.nn.conv2d(base_2, conv_w_bbox, 1),\n",
        "            conv_b_bbox\n",
        "        )\n",
        "\n",
        "        return [classtify, bbox]\n",
        "    def get_weights():\n",
        "        return [\n",
        "                conv_w_base_1,\n",
        "                conv_b_base_1,\n",
        "                conv_w_base_2,\n",
        "                conv_b_base_2,\n",
        "                conv_w_classtify,\n",
        "                conv_b_classtify,\n",
        "                conv_w_bbox,\n",
        "                conv_b_bbox\n",
        "            ]\n",
        "\n",
        "    return [pred, get_weights]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk7UzOlATbhc",
        "colab_type": "code",
        "outputId": "e2928bac-90f0-4afb-a481-8bec792fc83e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "vgg16=tf.keras.applications.VGG16()\n",
        "# vgg16.summary()\n",
        "vgg16_=tf.keras.models.Model(vgg16.layers[0].input, vgg16.layers[17].output)\n",
        "vgg16_.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 4s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cGUfoNGrgTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[rpn, get_rpn_weights] = RPN()\n",
        "[pred, get_pred_weights] = Pred()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV0YX9eefVBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg16_.predict\n",
        "rpn\n",
        "cull_anchor\n",
        "roi_pool\n",
        "pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLpOX9n26-0p",
        "colab_type": "text"
      },
      "source": [
        "# Loss 計算\n",
        "\n",
        "## anchor\n",
        "\n",
        "> 每張圖片經過特徵提取器後的大小會變成 [M, N, C]，  \n",
        "> 那麼 anchor 的總數量就會是 M * N * K，  \n",
        "> K 代表 anchor 的類別數量。\n",
        "\n",
        "* ### positive anchor\n",
        "\n",
        "    當 anchor 與 ground truth 之間的 IoU > 0.7，便將此 anchor 視為 positive anchor。\n",
        "\n",
        "* ### negative anchor\n",
        "\n",
        "    當 anchor 與 ground truth 之間的 IoU < 0.3，便將此 anchor 視為 negative anchor。"
      ]
    }
  ]
}